{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Recognition\n",
    "https://towardsdatascience.com/real-time-face-recognition-an-end-to-end-project-b738bb0f7348\n",
    "https://github.com/Mjrovai/OpenCV-Face-Recognition/blob/master/FaceDetection/faceDetection.py\n",
    "https://face-recognition.readthedocs.io/en/latest/face_recognition.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The objective is to use a Raspberry Pi 3+ and the camera for the Raspbery Pi and have it act as a security camera with face dection features. The Raspberry Pi should work headlessly, be able to collect faces for training and dectect people's faces that has been trained. We will be using the lastest version of Raspian as of 2/15/2021 as well as OpenCV and xxx. The majority of this walk through was gathered from different articles and videos that will be linked below. Although I didnt go through the OpenCV documentations, this was a good start and at some point I would like to do a complete OpenCV walk through.\n",
    "\n",
    "Limitations: Exploring OpenCV and the pi's camera was fun but there were some limitations. The pi's camera to wasn't able to capture people's faces from the distance that I wanted it to. The idea was to have it sit on a window sile and gather faces as people walked by. The pi had difficulty capturing people's faces from about 50 feet away and for the face capturing portion, your subject had to be still for several seconds looking directly at the camera. Best results came at about 2-3 feet, anything between 3-6 feet would be identified as \"Unknonw\" and at 6 feet, it wouldnt pick up any faces at all. I would like to try this again but with a high quality usb camera or with a better pi camera.\n",
    "\n",
    "This tutorial will be broken down in many parts listed below:\n",
    "\n",
    "1. [Video with Python](#1.Video-with-Python)\n",
    "2. [Facial Recognition with Video](#2.Facial-Recognition-with-Video)\n",
    "3. [Data Gathering](#3.Data-Gathering)\n",
    "4. [Training](#4.Training)\n",
    "5. [Recognizer](#5.Recognizer)\n",
    "\n",
    "\n",
    "Follow up:\n",
    " - From command line, show the face distance and adjust tolerance, @4:30 of youtube video\n",
    " - Go through OpenCV doc\n",
    " - Try same setup with usb camera to better capture training data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Video with Python\n",
    "[Top](#Objective)\n",
    "<br>\n",
    "<br>\n",
    "`pip3 install opencv-python`\n",
    "<br>\n",
    "and execute the following code. Here we are just initiating the camera, using opencv/\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) /tmp/pip-req-build-ddpkm6fn/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1e6380378249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#frame = cv2.flip(frame, -1) # Flip camera vertically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#sets the frame to gray sale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#this shows the \"frame\" frame from the above ret, frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.5.1) /tmp/pip-req-build-ddpkm6fn/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0) \n",
    "cap.set(3,640) # 3 set Width\n",
    "cap.set(4,480) # 4 set Height\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    #frame = cv2.flip(frame, -1) # Flip camera vertically\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #sets the frame to gray sale\n",
    "    \n",
    "    cv2.imshow('frame', frame) #this shows the \"frame\" frame from the above ret, frame\n",
    "    #cv2.imshow('gray', gray)  #this shows the converted gray scale frame. When we enable both we will show two frames, one in color one is grey scale\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff #dont know how this code works exactly but it kills the process when you press ESC\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code output above is pretty simple. We load the face_recognition package, find the location of the all faces in a picture that we pass using the .face_location function. The function returns a tuple of the location of the square boxes around each person's face,  (top, right, bottom, left). There are 5 faces so five items in our tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Facial Recognition with Video\n",
    "[Top](#Objective)\n",
    "<br>\n",
    "<br>\n",
    "To detect faces we need to use the Haar Cascade classifier. Using Haar feature-base we can detect faces as well as objects. You can create your own cascade classifier or use defualt ones. You can find an assortment of Haar classifiers here:\n",
    "\n",
    "https://github.com/opencv/opencv/tree/master/data/haarcascades\n",
    "<br>\n",
    "<br>\n",
    "1)Set opencv to capture video\n",
    "<br>\n",
    "2)Create loop that will read the frames\n",
    "<br>\n",
    "3)Change the frame to grayscale\n",
    "<br>\n",
    "4)Use a cascade to find the face\n",
    "<br>\n",
    "5)Draw rectangles around the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The faces do not match\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier('../haarcascade_frontalface_default.xml')\n",
    "\n",
    "#faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) # 3 set Width\n",
    "cap.set(4,480) # 4 set Height\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read() #Returns a bool T/F is frame is read correctly\n",
    "    #img = cv2.flip(img, -1)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #convert image to gray color space\n",
    "    faces = faceCascade.detectMultiScale(        #faceCascade OBJECT has a METHOD call detectMultiScale, this runs a classifier cascade over the image\n",
    "        gray,                                    #MultiScale algorithm looks at a subregion of an image in multiple scales to detect faces of varying size\n",
    "        scaleFactor=1.2,                         #How much to resize the image, lower the better, because you reduce the size of the image by a smaller amounmt, \n",
    "        minNeighbors=5,                                  #to match the size of the model, reduce it too much and you loose lots of data from the pic?     \n",
    "        minSize=(20, 20)                                  #minNeighbor says how many neighbor each candidate rectangle should retain. Higher value, fewer resutls but higher qaulity 3-6 is good\n",
    "    )                                                     #Min size of possible objects. Small than this we ignore. Return x,y, start loction, h hieight and w width\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)  #.rect(image, start.point, end.point, color, thickness)), how to make the rectangle\n",
    "        roi_gray = gray[y:y+h, x:x+w]  #dont know why indexing the gray image\n",
    "        roi_color = img[y:y+h, x:x+w]  #dont know why indexing of the color image\n",
    "    cv2.imshow('video',img)            #call cv2 to show, a windwing called video, and the img we cap.read() from above\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Data Gathering\n",
    "[Top](#Objective)\n",
    "<br>\n",
    "<br>\n",
    "Here we will be gathering the faces for so we can identify the faces. We are going to setup the camera to captures the individual faces and save them for training. !!!When using this script to gather faces, best to use number as the trainer is setup to use id numbers not names\n",
    "<br>\n",
    "<br>\n",
    "1)Set opencv to capture video\n",
    "<br>\n",
    "2)Create loop that will read the frames\n",
    "<br>\n",
    "3)Change the frame to grayscale\n",
    "<br>\n",
    "4)Use a cascade to find the face, then save all the faces to a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) # set Width\n",
    "cap.set(4,480) # set Height\n",
    "\n",
    "face_detector = cv2.CascadeClassifier('../haarcascade_frontalface_default.xml')\n",
    "\n",
    "#for each person, enter one numeric face id\n",
    "\n",
    "face_id = input('\\n enter user id end press <return> ==> ')\n",
    "print(\"\\n [INFO] Initializing face capture. Look at the camera and wait..\")\n",
    "\n",
    "count = 0\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    #img = cv2.flip(img, -1)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_detector.detectMultiScale(gray,1.3,5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        count +=1\n",
    "        #Save the captured image into the dataset foldera\n",
    "        cv2.imwrite(\"../face.sample/User.\"+str(face_id)+'.'+str(count)+\".jpg\",gray[y:y+h,x:x+w])\n",
    "        cv2.imshow('image',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "    elif count >= 30: #Take 30 face sample and stop video\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Training\n",
    "[Top](#Objective)\n",
    "<br>\n",
    "<br>\n",
    "Here we will be using .createLBPHFacesRecognizer and the .CascadeClassifier as we did in the previous example, more information about the two can be found here (https://stackoverflow.com/questions/8791178/haar-cascades-vs-lbp-cascades-in-face-detection). We are going to take faces we gathered above and match it to an image with unknown faces and hopefully it can identify the face in the unknonw image. It will return a trainer.yml that we will use later to recognize new faces. This works very much like the last we codes we ran:\n",
    "<br>\n",
    "1)Set opencv to capture video\n",
    "<br>\n",
    "2)Create loop that will read the frames\n",
    "<br>\n",
    "3)Change the frame to grayscale\n",
    "<br>\n",
    "4)Use a cascade to find the face\n",
    "<br>\n",
    "5)Draw rectangles around the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99205385 0.89515966]\n",
      "[0.48802559 0.75429731]\n",
      "[0.96776413 0.93438534]\n",
      "[0.79075103 0.50995554]\n",
      "[0.92704192 0.99004671]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path for face image database\n",
    "path = '../face.sample'\n",
    "recognizer = cv2.face.createLBPHFaceRecognizer() #Local Binary Patterns Histogram face recognizer in opencCV\n",
    "detector = cv2.CascadeClassifier(\"../haarcascade_frontalface_default.xml\") #detect face or any objects\n",
    "\n",
    "# function to get the images and label data\n",
    "def getImagesAndLabels(path):\n",
    "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)] #Creates a list of each image and its path\n",
    "    faceSamples=[]                  #os.path.join() will take the path variable which is our parent path (../face.sample) to our currnet directory \n",
    "    ids = []                        #Then combines it with with each of the files in the parent path using os.listdir, ../face.sample\n",
    "    for imagePath in imagePaths:\n",
    "        PIL_img = Image.open(imagePath).convert('L') # Convert image in from path to a PIL image format, here convert to gray scale \"L\"\n",
    "        img_numpy = np.array(PIL_img,'uint8')   #Convert the PIL image to a numpy unit8 array\n",
    "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])  #os.path.split(), splits the path into the directory path, and the file. \n",
    "        faces = detector.detectMultiScale(img_numpy)   #from os.path.split() we take the filename, [-1], then split the file name and take the [1] position, which is the id we gave it before\n",
    "        for (x,y,w,h) in faces:    #detector.detectMultiScale() is simialr to face_recognition.face_location in that it return x,y,w,h top, right, left bottom\n",
    "            faceSamples.append(img_numpy[y:y+h,x:x+w])  #the location from faces is now applied to the numpy image, uint8\n",
    "            ids.append(id)  #then append the id we got from the split os path for id\n",
    "    return faceSamples,ids\n",
    "\n",
    "print (\"\\n [INFO] Training faces. It will take a few seconds. Wait ...\")\n",
    "\n",
    "faces,ids = getImagesAndLabels(path)  #from our function we assign what is returned to faces and ids\n",
    "recognizer.train(faces, np.array(ids)) #then train the faces to ids?\n",
    "\n",
    "# Save the model into trainer/trainer.yml\n",
    "recognizer.save('../trainer.yml') \n",
    "\n",
    "# Print the numer of faces trained and end program\n",
    "print(\"\\n [INFO] {0} faces trained. Exiting Program\".format(len(np.unique(ids))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Recognizer\n",
    "[Top](#Objective)\n",
    "<br>\n",
    "<br>\n",
    "Finally we will capture a face and have our recognizer make a prediction and returns its id and index.\n",
    "<br>\n",
    "<br>\n",
    "0) Initialize video capture using cv2\n",
    "<br>\n",
    "1) Load image file of known (bruno)\n",
    "<br>\n",
    "2) Encode the face to get a list of 128-dimensional face encodings (bruno)\n",
    "<br>\n",
    "3) Assign the face encoding names (bruno)\n",
    "<br>\n",
    "4) Start a \"loop\" using while to capture all the frames from the camera\n",
    "<br>\n",
    "5) Get face location for the unknown face using face_recognition.face_locations()\n",
    "<br>\n",
    "6) Encode the unknown faces using the location from #5 with .face_encoding()\n",
    "<br>\n",
    "7) Create a for loop to iterate throught each video frame\n",
    "<br>\n",
    "8) Check to see if the face match\n",
    "<br>\n",
    "9) Set a rectangle around the face with name\n",
    "<br>\n",
    "https://www.youtube.com/watch?v=lC_y8wD7F3Y&t=797s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-e988330bd7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#https://github.com/bertcenteno/face_recognition_video_tutorial/blob/master/face_reco_webcam.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "#Issues you ran into\n",
    "#1 when you curled the cascade, it did not download completely, you had to go to the website and dl and extract\n",
    "#the cv2.face.LBPHFaceRecognizer() we either an older or new package, had to use cv2.face.createLBPHFaceRecognizer()\n",
    "#with that we had to use save instead of write and load instead of read\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "recognizer = cv2.face.createLBPHFaceRecognizer() #Local Binary Patterns Histogram face recognizer in opencCV. Assigns recognizer to a face recognizer\n",
    "recognizer.load('../trainer.yml') #loads the trainer.yml file we got from above to recognizer we just assign\n",
    "cascadePath = \"../haarcascade_frontalface_default.xml\" #set cascade xml path\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath); #like we did above for dectector\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX #sets the font\n",
    "\n",
    "#iniciate id counter\n",
    "id = 0\n",
    "\n",
    "# names related to ids: example ==> Marcelo: id=1,  etc\n",
    "names = ['Linc','hman'] \n",
    "\n",
    "# Initialize and start realtime video capture\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3, 640) # set video widht\n",
    "cam.set(4, 480) # set video height\n",
    "\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1*cam.get(3)\n",
    "minH = 0.1*cam.get(4)\n",
    "\n",
    "while True:\n",
    "    ret, img =cam.read()\n",
    "    #img = cv2.flip(img, -1) # Flip vertically\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #convert image to gray color space\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(  #https://stackoverflow.com/questions/36218385/parameters-of-detectmultiscale-in-opencv-using-python\n",
    "        gray,                                    #MultiScale algorithm looks at a subregion of an image in multiple scales to detect faces of varying size\n",
    "        scaleFactor=1.2,                         #How much to resize the image, lower the better, because you reduce the size of the image by a smaller amounmt, \n",
    "        minNeighbors=5,                          #to match the size of the model, reduce it too much and you loose lots of data from the pic?     \n",
    "        minSize = (int(minW), int(minH))         #minNeighbor says how many neighbor each candidate rectangle should retain. Higher value, fewer resutls but higher qaulity 3-6 is good                                                     \n",
    "                )                                #Min size of possible objects. Small than this we ignore. Return x,y, start loction, h hieight and w width\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2) #.rect(image, start.point, end.point, color, thickness)), how to make the rectangle\n",
    "        id, confidence = recognizer.predict(gray[y:y+h,x:x+w]) #the recognizer will predict the image and its ide and return the id and confidenc\n",
    "     \n",
    "        # If confidence is less them 100 ==> \"0\" : perfect match \n",
    "        if (confidence < 100):\n",
    "            id = names[id]\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        else:\n",
    "            id = \"unknown\"\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        \n",
    "        cv2.putText(              #Put text around the square we made to find the faces\n",
    "                    img,          #Image we've put the box around\n",
    "                    str(id),      #The id we got from the if statements, unknow or unknown\n",
    "                    (x+5,y-5),   #The position to put the ext\n",
    "                    font,        #Font type as describe above\n",
    "                    1,           #Fontscale\n",
    "                    (255,255,255), #Color\n",
    "                    2              #Thickness\n",
    "                   )\n",
    "        cv2.putText(\n",
    "                    img, \n",
    "                    str(confidence), \n",
    "                    (x+5,y+h-5), \n",
    "                    font, \n",
    "                    1, \n",
    "                    (255,255,0), \n",
    "                    1\n",
    "                   )  \n",
    "    \n",
    "    cv2.imshow('camera',img) #name of the window and the img to show\n",
    "    k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Do a bit of cleanup\n",
    "print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### #.asdf\n",
    "[Top](#Objective)\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
